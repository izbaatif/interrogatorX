{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "            <p> We will first start by importing the libraries required and import our dataset. The data may have some empty fields and those will need to be removed to get a better model. The dataset used is the following : <a href=\"https://www.kaggle.com/datasets/jruvika/fake-news-detection\"> fake_news.csv </a></p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data is dataframe\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extra columns\n",
    "df = df[['Headline' , 'Body' , 'Label']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty rows\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting tokens\n",
    "def get_words(text):\n",
    "    #make tokens of everything in the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    #lemmatize and remove stop words here in future\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the function on a sentence \n",
    "get_words(\"email healthcare reform to make america great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 80:20 for training and testing\n",
    "#find 80th percent of the length of the dataframe\n",
    "split = int(len(df) * 0.8)\n",
    "\n",
    "#split everything before that into train and the rest into test\n",
    "train_df, test_df = df[:split], df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary which stores each token and its count\n",
    "# Empty dictionary for storing the tokens\n",
    "token_count = {}\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "for text in train_df['Body']:\n",
    "    tokens = get_words(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in token_count:\n",
    "            token_count[token] += 1\n",
    "        else:\n",
    "            token_count[token] = 1\n",
    "\n",
    "# Print the count of unique tokens\n",
    "print(len(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the tokens with the count higher than specified number\n",
    "def common_tokens(token, number):\n",
    "    if token not in token_count:\n",
    "        return False\n",
    "    else:\n",
    "        tokens = token_count[token] > number\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test it\n",
    "common_tokens(\"government\" , 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the returned words in set\n",
    "common = set()\n",
    "\n",
    "#loops through the token count dictionary \n",
    "for token in token_count:\n",
    "    #if the value if greater than 700 adds it to ser\n",
    "    if common_tokens(token,700):\n",
    "        common.add(token)\n",
    "        \n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give them an index from most to least appeared\n",
    "#convert to list\n",
    "common = list(common)\n",
    "\n",
    "#map them\n",
    "mapping = {t:i for t,i in zip(common, range(len(common)))}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bag of words\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    #make an empty vector of the length of common list\n",
    "    count_vector = np.zeros(len(common))\n",
    "    \n",
    "    #get the tokens list and store them\n",
    "    proc_tokens = get_words(text)\n",
    "    \n",
    "    #count them and add them to the vector\n",
    "    for token in proc_tokens:\n",
    "        #if not in common, ignore the tokens \n",
    "        if token not in common:\n",
    "            continue\n",
    "        #count the values\n",
    "        index = mapping[token]\n",
    "        count_vector[index] += 1\n",
    "        \n",
    "    return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the label and text and apply bag of words function to it\n",
    "def df_xy(dataframe):\n",
    "    y = dataframe['Label'].to_numpy().astype(int)\n",
    "    \n",
    "    text = dataframe['Body']\n",
    "    vectors = []\n",
    "    \n",
    "    for t in text:\n",
    "        vector = bag_of_words(t)\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    x = np.array(vectors).astype(int)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test using previous function\n",
    "x_train, y_train = df_xy(train_df)\n",
    "x_test, y_test = df_xy(test_df)\n",
    "\n",
    "\n",
    "x_train.shape , y_train.shape , x_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
