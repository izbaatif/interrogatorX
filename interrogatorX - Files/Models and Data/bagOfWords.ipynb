{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCARY! LEAKED EMAIL PROVES Radical Billionaire...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch as Assad Destroys US Reporter Michael Is...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK counter-terrorism police charge 14-year-old...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Internet Drags Trump’s Son For Saying ‘Th...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Koch Has The Sads Because He Thinks H...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  SCARY! LEAKED EMAIL PROVES Radical Billionaire...  fake\n",
       "1  Watch as Assad Destroys US Reporter Michael Is...  fake\n",
       "2  UK counter-terrorism police charge 14-year-old...  real\n",
       "3   The Internet Drags Trump’s Son For Saying ‘Th...  fake\n",
       "4   Charles Koch Has The Sads Because He Thinks H...  fake"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save data in dataframe\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#get all the English stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which will first tokenize all the words in the Body field. \n",
    "#Then these words will be lemmatized to obtain the root words for example dance and dancing will be the same. \n",
    "#After that it will remove the stop words such as 'the' to only keep important words which are useful for the analysis. \n",
    "#Lastly, this function will return these words in a list.\n",
    "def preprocess_text(text):\n",
    "    # Check if text is a string or can be converted to a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Make tokens of everything in the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Make them all lowercase\n",
    "    lower = [text.lower() for text in tokens]\n",
    "    \n",
    "    # Lemmatize all the tokens and store them in a list\n",
    "    lemmatized = [lemmatizer.lemmatize(text) for text in lower]\n",
    "    \n",
    "    # Get all the words which aren't stopwords in a list\n",
    "    words = [text for text in lemmatized if text not in stopwords]\n",
    "    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 80:20 for train and test\n",
    "#find 80th percent of the length of the dataframe\n",
    "split = int(len(df) * 0.8)\n",
    "\n",
    "#split everything before that into train and the rest into test\n",
    "train_df, test_df = df[:split], df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125962\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary which stores each token and its count\n",
    "# Empty dictionary for storing the tokens\n",
    "token_count = {}\n",
    "\n",
    "# Tokenize the combined text\n",
    "for text in train_df['Text']:\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in token_count:\n",
    "            token_count[token] += 1\n",
    "        else:\n",
    "            token_count[token] = 1\n",
    "\n",
    "# Print the count of unique tokens\n",
    "print(len(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140366\n"
     ]
    }
   ],
   "source": [
    "#check max and min value\n",
    "print(max(token_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min(token_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which returns true if input word is appearing between the threshold\n",
    "def common_tokens(token, lowthresh, highthresh):\n",
    "    if token not in token_count:\n",
    "        return False\n",
    "    else:\n",
    "        tokens = token_count[token] > lowthresh and token_count[token] < highthresh\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'victory', 'thing', 'texas', 'much', 'international', 'plan', 'capital', 'judge', 'record', 'donald', 'everyone', 'ryan', 'crisis', 'intelligence', 'given', 'community', 'known', 'others', 'service', 'message', 'army', 'militant', 'shooting', 'detail', 'special', 'wall', 'wrong', 'play', 'adviser', 'photo', 'went', 'interest', 'seems', 'opposition', 'example', 'speech', 'first', 'county', '4', 'term', 'n', 'center', 'muslim', 'facebook', 'group', 'point', 'committee', 'may', 'either', 'key', 'three', 'primary', 'november', 'simply', 'meeting', 'presidential', 'making', 'six', 'healthcare', 'convention', 'found', 'despite', 'expected', 'made', 'post', 'sander', 'well', 'even', 'view', 'al', 'fact', 'able', 'set', 'already', 'democratic', 'mexico', 'statement', 'energy', 'account', 'book', 'protest', 'secretary', 'weapon', 'hit', 'reported', '2013', 'obama', 'together', 'better', 'hand', 'work', 'increase', 'today', 'east', 'half', 'europe', 'never', 'failed', 'several', 'potential', 'important', 'fire', 'united', 'real', 'lost', 'whose', 'general', 'number', 'mark', 'letter', 'small', 'barack', 'california', 'week', 'home', 'power', 'leading', 'talking', 'late', 'voted', 'according', 'interview', 'reporter', 'world', 'vote', 'hold', 'must', 'reuters', 'law', 'change', 'help', '100', 'terrorist', 'received', 'morning', 'challenge', 'tweet', 'iraq', 'spokesman', 'syrian', 'third', 'accused', 'union', 'individual', 'next', 'conservative', 'turkey', 'financial', 'concern', 'price', 'report', 'administration', 'climate', 'tuesday', 'line', 'foundation', 'came', 'right', 'fighting', 'sen', 'director', 'ban', 'five', 'man', 'movement', 'cut', '30', 'terrorism', 'began', 'government', 'hearing', 'nomination', 'south', 'christian', 'took', 'wrote', 'business', 'russian', 'would', 'college', 'make', 'full', 'racist', 'trying', 'non', 'sure', 'friend', 'student', '3', 'latest', 'bill', 'taking', 'recent', 'george', 'though', 'congressional', 'allegation', 'enough', 'mr', 'worker', 'young', 'official', 'hate', 'new', 'economy', 'health', 'building', 'money', 'order', 'thousand', 'low', 'military', 'party', 'chinese', 'toward', 'ago', '2', 'former', 'r', 'doe', 'left', 'tell', 'away', 'fbi', 'serious', 'operation', 'guy', 'near', '2017', 'iran', 'nominee', 'featured', 'policy', 'turn', 'sunday', 'poll', 'isi', 'response', 'actually', 'life', 'percent', 'face', 'opinion', 'entire', 'trade', 'month', 'held', 'daily', 'case', 'tried', 'shot', 'class', 'tax', 'liberal', 'board', 'august', 'obamacare', 'candidate', 'likely', 'father', 'effort', 'like', 'september', 'people', 'supporter', 'u', 'short', 'chief', 'lot', 'far', 'forward', 'course', 'session', 'one', 'following', 'attorney', 'incident', 'russia', 'sanction', 'different', 'also', 'rate', 'based', 'year', 'many', 'information', 'fake', 'cannot', 'hour', 'decided', 'host', 'high', 'staff', 'islamic', 'car', 'video', 'pro', 'added', 'hard', 'senator', 'area', 'show', 'happened', 'front', 'little', 'running', 'four', 'anti', 'story', 'representative', 'drug', 'civil', 'care', 'address', 'moscow', 'conference', 'get', 'situation', 'data', 'although', 'white', 'eu', 'around', 'live', 'conflict', 'open', 'family', 'republican', 'find', 'got', 'food', 'race', 'pic', 'place', 'word', 'taken', 'travel', 'border', 'body', 'best', 'black', 'someone', 'gave', 'level', 'legislation', 'korea', 'friday', 'university', 'personal', 'done', 'freedom', 'senior', 'back', 'council', 'fund', 'nuclear', 'strong', 'long', 'country', 'industry', 'immigration', 'police', 'free', 'political', 'need', '9', 'department', 'spending', 'later', 'network', 'ministry', 'minister', 'violence', '11', 'city', 'something', 'town', 'used', 'involved', 'foreign', 'yet', 'saturday', 'due', 'leave', '6', 'website', 'current', 'february', 'keep', 'talk', 'nothing', 'citizen', 'private', 'european', 'company', 'request', 'agent', 'april', 'looking', 'proposal', 'israel', 'news', 'wife', 'medium', 'worked', 'cost', 'vice', 'bush', 'north', 'pressure', 'kind', 'room', 'release', 'son', 'attack', 'include', 'win', 'syria', 'risk', 'planned', 'fight', 'test', 'another', 'showed', 'debate', 'gun', 'along', 'public', 'street', 'relationship', 'action', 'article', 'anyone', 'within', 'elected', 'election', 'chairman', 'global', 'final', 'dollar', 'speaker', 'mean', 'met', 'force', 'problem', 'provide', 'project', 'cruz', 'court', 'president', '1', 'agency', 'love', 'james', 'agreement', 'act', 'last', 'early', 'clear', 'put', 'go', 'answer', 'ally', 'reform', 'step', 'day', 'let', 'among', 'sent', 'http', 'air', 'british', 'korean', 'decade', 'stop', 'america', 'child', 'member', 'using', 'email', 'still', 'main', 'form', 'speaking', 'britain', 'star', 'meet', 'congress', 'wanted', 'always', 'june', 'end', 'run', 'working', 'belief', 'support', 'result', 'especially', 'game', '000', '10', 'released', 'claim', 'parliament', 'move', 'know', 'putin', 'prime', 'budget', 'fear', 'executive', 'time', 'issue', 'needed', 'politics', 'human', 'gop', 'ground', 'choice', 'lie', 'campaign', 'october', 'called', 'position', 'continue', 'saudi', 'lawmaker', '25', 'co', '15', 'two', 'however', 'might', 'without', 'organization', 'john', 'deal', 'washington', 'thursday', 'say', 'top', 'lawyer', 'good', 'whether', 'ever', 'attempt', 'illegal', '50', 'death', 'flag', 'crime', 'think', 'aide', 'look', 'january', 'western', 'legal', 'recently', 'comment', 'central', 'december', 'nation', 'victim', 'district', 'charge', 'immigrant', 'process', 'head', 'hundred', 'cnn', 'bad', 'florida', 'refugee', 'return', 'call', 'officer', 'press', '2014', 'ha', 'realdonaldtrump', 'water', 'bring', '12', 'presidency', 'truth', 'wednesday', 'majority', 'including', 'want', 'visit', 'war', 'paul', 'use', 'history', 'local', 'anything', 'moment', 'probably', 'big', '5', 'biggest', 'school', 'sexual', 'outside', 'feel', 'asked', 'fox', 'firm', 'protester', 'region', 'voter', 'governor', 'led', 'supreme', 'men', 'threat', 'york', 'give', 'list', 'national', 'least', 'peace', 'page', 'going', 'event', 'decision', 'march', 'start', 'started', 'close', 'since', 'saying', 'review', 'crowd', 'rally', 'le', 'idea', 'told', 'rule', 'cause', '8', 'state', 'leader', 'missile', 'germany', 'justice', 'source', 'getty', 'coalition', 'great', 'future', 'instead', 'calling', 'everything', 'try', 'access', 'target', 'house', 'criminal', 'take', 'could', 'measure', 'side', 'part', 'security', 'funding', 'old', '2012', 'million', 'every', 'behind', 'bernie', 'economic', 'oil', 'often', 'role', 'relation', 'hillary', 'program', 'voting', 'twitter', '2015', 'system', 'across', 'believe', 'seen', 'senate', 'employee', 'see', 'politician', 'rubio', 'july', 'immediately', 'investigation', '20', 'rather', 'com', 'defense', 'democrat', 'way', 'west', '7', 'woman', 'benefit', 'soon', 'office', 'backed', 'true', 'coming', 'announced', 'expert', 'federal', 'lead', 'stand', 'authority', 'come', 'chance', 'document', 'secret', 'j', 'sign', 'allowed', 'name', 'person', 'getting', 'second', 'clinton', 'nearly', 'ahead', 'hope', 'monday', 'china', 'question', 'matter', 'billion', '2016', 'read', 'night', 'activist', 'allow', 'team', 'killed', 'ruling', 'protect', 'possible', 'social', 'major', 'middle', 'large', 'almost', 'control', 'share', 'pay', 'watch', 'earlier', 'past', 'job', 'bank', 'market', 'tv', 'thought', 'become', 'independent', 'reason', 'tie', 'really', 'american', 'evidence', 'century', 'via', 'enforcement', 'image', 'seek', 'comey'}\n"
     ]
    }
   ],
   "source": [
    "#set to store all tokens apperaing inside threshold\n",
    "common = set()\n",
    "\n",
    "#loops through the token count dictionary \n",
    "for token in token_count:\n",
    "    #if the value if greater than 3000 and less than 100000 adds it \n",
    "    if common_tokens(token,3000, 100000):\n",
    "        common.add(token)\n",
    "        \n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'victory': 0, 'thing': 1, 'texas': 2, 'much': 3, 'international': 4, 'plan': 5, 'capital': 6, 'judge': 7, 'record': 8, 'donald': 9, 'everyone': 10, 'ryan': 11, 'crisis': 12, 'intelligence': 13, 'given': 14, 'community': 15, 'known': 16, 'others': 17, 'service': 18, 'message': 19, 'army': 20, 'militant': 21, 'shooting': 22, 'detail': 23, 'special': 24, 'wall': 25, 'wrong': 26, 'play': 27, 'adviser': 28, 'photo': 29, 'went': 30, 'interest': 31, 'seems': 32, 'opposition': 33, 'example': 34, 'speech': 35, 'first': 36, 'county': 37, '4': 38, 'term': 39, 'n': 40, 'center': 41, 'muslim': 42, 'facebook': 43, 'group': 44, 'point': 45, 'committee': 46, 'may': 47, 'either': 48, 'key': 49, 'three': 50, 'primary': 51, 'november': 52, 'simply': 53, 'meeting': 54, 'presidential': 55, 'making': 56, 'six': 57, 'healthcare': 58, 'convention': 59, 'found': 60, 'despite': 61, 'expected': 62, 'made': 63, 'post': 64, 'sander': 65, 'well': 66, 'even': 67, 'view': 68, 'al': 69, 'fact': 70, 'able': 71, 'set': 72, 'already': 73, 'democratic': 74, 'mexico': 75, 'statement': 76, 'energy': 77, 'account': 78, 'book': 79, 'protest': 80, 'secretary': 81, 'weapon': 82, 'hit': 83, 'reported': 84, '2013': 85, 'obama': 86, 'together': 87, 'better': 88, 'hand': 89, 'work': 90, 'increase': 91, 'today': 92, 'east': 93, 'half': 94, 'europe': 95, 'never': 96, 'failed': 97, 'several': 98, 'potential': 99, 'important': 100, 'fire': 101, 'united': 102, 'real': 103, 'lost': 104, 'whose': 105, 'general': 106, 'number': 107, 'mark': 108, 'letter': 109, 'small': 110, 'barack': 111, 'california': 112, 'week': 113, 'home': 114, 'power': 115, 'leading': 116, 'talking': 117, 'late': 118, 'voted': 119, 'according': 120, 'interview': 121, 'reporter': 122, 'world': 123, 'vote': 124, 'hold': 125, 'must': 126, 'reuters': 127, 'law': 128, 'change': 129, 'help': 130, '100': 131, 'terrorist': 132, 'received': 133, 'morning': 134, 'challenge': 135, 'tweet': 136, 'iraq': 137, 'spokesman': 138, 'syrian': 139, 'third': 140, 'accused': 141, 'union': 142, 'individual': 143, 'next': 144, 'conservative': 145, 'turkey': 146, 'financial': 147, 'concern': 148, 'price': 149, 'report': 150, 'administration': 151, 'climate': 152, 'tuesday': 153, 'line': 154, 'foundation': 155, 'came': 156, 'right': 157, 'fighting': 158, 'sen': 159, 'director': 160, 'ban': 161, 'five': 162, 'man': 163, 'movement': 164, 'cut': 165, '30': 166, 'terrorism': 167, 'began': 168, 'government': 169, 'hearing': 170, 'nomination': 171, 'south': 172, 'christian': 173, 'took': 174, 'wrote': 175, 'business': 176, 'russian': 177, 'would': 178, 'college': 179, 'make': 180, 'full': 181, 'racist': 182, 'trying': 183, 'non': 184, 'sure': 185, 'friend': 186, 'student': 187, '3': 188, 'latest': 189, 'bill': 190, 'taking': 191, 'recent': 192, 'george': 193, 'though': 194, 'congressional': 195, 'allegation': 196, 'enough': 197, 'mr': 198, 'worker': 199, 'young': 200, 'official': 201, 'hate': 202, 'new': 203, 'economy': 204, 'health': 205, 'building': 206, 'money': 207, 'order': 208, 'thousand': 209, 'low': 210, 'military': 211, 'party': 212, 'chinese': 213, 'toward': 214, 'ago': 215, '2': 216, 'former': 217, 'r': 218, 'doe': 219, 'left': 220, 'tell': 221, 'away': 222, 'fbi': 223, 'serious': 224, 'operation': 225, 'guy': 226, 'near': 227, '2017': 228, 'iran': 229, 'nominee': 230, 'featured': 231, 'policy': 232, 'turn': 233, 'sunday': 234, 'poll': 235, 'isi': 236, 'response': 237, 'actually': 238, 'life': 239, 'percent': 240, 'face': 241, 'opinion': 242, 'entire': 243, 'trade': 244, 'month': 245, 'held': 246, 'daily': 247, 'case': 248, 'tried': 249, 'shot': 250, 'class': 251, 'tax': 252, 'liberal': 253, 'board': 254, 'august': 255, 'obamacare': 256, 'candidate': 257, 'likely': 258, 'father': 259, 'effort': 260, 'like': 261, 'september': 262, 'people': 263, 'supporter': 264, 'u': 265, 'short': 266, 'chief': 267, 'lot': 268, 'far': 269, 'forward': 270, 'course': 271, 'session': 272, 'one': 273, 'following': 274, 'attorney': 275, 'incident': 276, 'russia': 277, 'sanction': 278, 'different': 279, 'also': 280, 'rate': 281, 'based': 282, 'year': 283, 'many': 284, 'information': 285, 'fake': 286, 'cannot': 287, 'hour': 288, 'decided': 289, 'host': 290, 'high': 291, 'staff': 292, 'islamic': 293, 'car': 294, 'video': 295, 'pro': 296, 'added': 297, 'hard': 298, 'senator': 299, 'area': 300, 'show': 301, 'happened': 302, 'front': 303, 'little': 304, 'running': 305, 'four': 306, 'anti': 307, 'story': 308, 'representative': 309, 'drug': 310, 'civil': 311, 'care': 312, 'address': 313, 'moscow': 314, 'conference': 315, 'get': 316, 'situation': 317, 'data': 318, 'although': 319, 'white': 320, 'eu': 321, 'around': 322, 'live': 323, 'conflict': 324, 'open': 325, 'family': 326, 'republican': 327, 'find': 328, 'got': 329, 'food': 330, 'race': 331, 'pic': 332, 'place': 333, 'word': 334, 'taken': 335, 'travel': 336, 'border': 337, 'body': 338, 'best': 339, 'black': 340, 'someone': 341, 'gave': 342, 'level': 343, 'legislation': 344, 'korea': 345, 'friday': 346, 'university': 347, 'personal': 348, 'done': 349, 'freedom': 350, 'senior': 351, 'back': 352, 'council': 353, 'fund': 354, 'nuclear': 355, 'strong': 356, 'long': 357, 'country': 358, 'industry': 359, 'immigration': 360, 'police': 361, 'free': 362, 'political': 363, 'need': 364, '9': 365, 'department': 366, 'spending': 367, 'later': 368, 'network': 369, 'ministry': 370, 'minister': 371, 'violence': 372, '11': 373, 'city': 374, 'something': 375, 'town': 376, 'used': 377, 'involved': 378, 'foreign': 379, 'yet': 380, 'saturday': 381, 'due': 382, 'leave': 383, '6': 384, 'website': 385, 'current': 386, 'february': 387, 'keep': 388, 'talk': 389, 'nothing': 390, 'citizen': 391, 'private': 392, 'european': 393, 'company': 394, 'request': 395, 'agent': 396, 'april': 397, 'looking': 398, 'proposal': 399, 'israel': 400, 'news': 401, 'wife': 402, 'medium': 403, 'worked': 404, 'cost': 405, 'vice': 406, 'bush': 407, 'north': 408, 'pressure': 409, 'kind': 410, 'room': 411, 'release': 412, 'son': 413, 'attack': 414, 'include': 415, 'win': 416, 'syria': 417, 'risk': 418, 'planned': 419, 'fight': 420, 'test': 421, 'another': 422, 'showed': 423, 'debate': 424, 'gun': 425, 'along': 426, 'public': 427, 'street': 428, 'relationship': 429, 'action': 430, 'article': 431, 'anyone': 432, 'within': 433, 'elected': 434, 'election': 435, 'chairman': 436, 'global': 437, 'final': 438, 'dollar': 439, 'speaker': 440, 'mean': 441, 'met': 442, 'force': 443, 'problem': 444, 'provide': 445, 'project': 446, 'cruz': 447, 'court': 448, 'president': 449, '1': 450, 'agency': 451, 'love': 452, 'james': 453, 'agreement': 454, 'act': 455, 'last': 456, 'early': 457, 'clear': 458, 'put': 459, 'go': 460, 'answer': 461, 'ally': 462, 'reform': 463, 'step': 464, 'day': 465, 'let': 466, 'among': 467, 'sent': 468, 'http': 469, 'air': 470, 'british': 471, 'korean': 472, 'decade': 473, 'stop': 474, 'america': 475, 'child': 476, 'member': 477, 'using': 478, 'email': 479, 'still': 480, 'main': 481, 'form': 482, 'speaking': 483, 'britain': 484, 'star': 485, 'meet': 486, 'congress': 487, 'wanted': 488, 'always': 489, 'june': 490, 'end': 491, 'run': 492, 'working': 493, 'belief': 494, 'support': 495, 'result': 496, 'especially': 497, 'game': 498, '000': 499, '10': 500, 'released': 501, 'claim': 502, 'parliament': 503, 'move': 504, 'know': 505, 'putin': 506, 'prime': 507, 'budget': 508, 'fear': 509, 'executive': 510, 'time': 511, 'issue': 512, 'needed': 513, 'politics': 514, 'human': 515, 'gop': 516, 'ground': 517, 'choice': 518, 'lie': 519, 'campaign': 520, 'october': 521, 'called': 522, 'position': 523, 'continue': 524, 'saudi': 525, 'lawmaker': 526, '25': 527, 'co': 528, '15': 529, 'two': 530, 'however': 531, 'might': 532, 'without': 533, 'organization': 534, 'john': 535, 'deal': 536, 'washington': 537, 'thursday': 538, 'say': 539, 'top': 540, 'lawyer': 541, 'good': 542, 'whether': 543, 'ever': 544, 'attempt': 545, 'illegal': 546, '50': 547, 'death': 548, 'flag': 549, 'crime': 550, 'think': 551, 'aide': 552, 'look': 553, 'january': 554, 'western': 555, 'legal': 556, 'recently': 557, 'comment': 558, 'central': 559, 'december': 560, 'nation': 561, 'victim': 562, 'district': 563, 'charge': 564, 'immigrant': 565, 'process': 566, 'head': 567, 'hundred': 568, 'cnn': 569, 'bad': 570, 'florida': 571, 'refugee': 572, 'return': 573, 'call': 574, 'officer': 575, 'press': 576, '2014': 577, 'ha': 578, 'realdonaldtrump': 579, 'water': 580, 'bring': 581, '12': 582, 'presidency': 583, 'truth': 584, 'wednesday': 585, 'majority': 586, 'including': 587, 'want': 588, 'visit': 589, 'war': 590, 'paul': 591, 'use': 592, 'history': 593, 'local': 594, 'anything': 595, 'moment': 596, 'probably': 597, 'big': 598, '5': 599, 'biggest': 600, 'school': 601, 'sexual': 602, 'outside': 603, 'feel': 604, 'asked': 605, 'fox': 606, 'firm': 607, 'protester': 608, 'region': 609, 'voter': 610, 'governor': 611, 'led': 612, 'supreme': 613, 'men': 614, 'threat': 615, 'york': 616, 'give': 617, 'list': 618, 'national': 619, 'least': 620, 'peace': 621, 'page': 622, 'going': 623, 'event': 624, 'decision': 625, 'march': 626, 'start': 627, 'started': 628, 'close': 629, 'since': 630, 'saying': 631, 'review': 632, 'crowd': 633, 'rally': 634, 'le': 635, 'idea': 636, 'told': 637, 'rule': 638, 'cause': 639, '8': 640, 'state': 641, 'leader': 642, 'missile': 643, 'germany': 644, 'justice': 645, 'source': 646, 'getty': 647, 'coalition': 648, 'great': 649, 'future': 650, 'instead': 651, 'calling': 652, 'everything': 653, 'try': 654, 'access': 655, 'target': 656, 'house': 657, 'criminal': 658, 'take': 659, 'could': 660, 'measure': 661, 'side': 662, 'part': 663, 'security': 664, 'funding': 665, 'old': 666, '2012': 667, 'million': 668, 'every': 669, 'behind': 670, 'bernie': 671, 'economic': 672, 'oil': 673, 'often': 674, 'role': 675, 'relation': 676, 'hillary': 677, 'program': 678, 'voting': 679, 'twitter': 680, '2015': 681, 'system': 682, 'across': 683, 'believe': 684, 'seen': 685, 'senate': 686, 'employee': 687, 'see': 688, 'politician': 689, 'rubio': 690, 'july': 691, 'immediately': 692, 'investigation': 693, '20': 694, 'rather': 695, 'com': 696, 'defense': 697, 'democrat': 698, 'way': 699, 'west': 700, '7': 701, 'woman': 702, 'benefit': 703, 'soon': 704, 'office': 705, 'backed': 706, 'true': 707, 'coming': 708, 'announced': 709, 'expert': 710, 'federal': 711, 'lead': 712, 'stand': 713, 'authority': 714, 'come': 715, 'chance': 716, 'document': 717, 'secret': 718, 'j': 719, 'sign': 720, 'allowed': 721, 'name': 722, 'person': 723, 'getting': 724, 'second': 725, 'clinton': 726, 'nearly': 727, 'ahead': 728, 'hope': 729, 'monday': 730, 'china': 731, 'question': 732, 'matter': 733, 'billion': 734, '2016': 735, 'read': 736, 'night': 737, 'activist': 738, 'allow': 739, 'team': 740, 'killed': 741, 'ruling': 742, 'protect': 743, 'possible': 744, 'social': 745, 'major': 746, 'middle': 747, 'large': 748, 'almost': 749, 'control': 750, 'share': 751, 'pay': 752, 'watch': 753, 'earlier': 754, 'past': 755, 'job': 756, 'bank': 757, 'market': 758, 'tv': 759, 'thought': 760, 'become': 761, 'independent': 762, 'reason': 763, 'tie': 764, 'really': 765, 'american': 766, 'evidence': 767, 'century': 768, 'via': 769, 'enforcement': 770, 'image': 771, 'seek': 772, 'comey': 773}\n"
     ]
    }
   ],
   "source": [
    "#The words are then added to a list and arranged and given an index of most to least appeared\n",
    "#convert to list\n",
    "common = list(common)\n",
    "\n",
    "#map them\n",
    "mapping = {t:i for t,i in zip(common, range(len(common)))}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> Now make a bag of words. Bag of words include the words and how many times it appears in a text. For example, in the text: \"this year trump won the elections at this time of the year.\" : </p>\n",
    "        <p> [\"trump\" , \"year\" , \"time\"] <br>[  1  ,   2   ,   1  ]\n",
    "        </p>\n",
    "        <p> We will make a function to create the bag of words now. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bag of words\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    #make an empty vector of the length of common list\n",
    "    count_vector = np.zeros(len(common))\n",
    "    \n",
    "    #get the tokens list and store them\n",
    "    proc_tokens = preprocess_text(text)\n",
    "    \n",
    "    #count them and add them to the vector\n",
    "    for token in proc_tokens:\n",
    "        #if not in common, ignore the tokens \n",
    "        if token not in common:\n",
    "            continue\n",
    "        #count the values\n",
    "        index = mapping[token]\n",
    "        count_vector[index] += 1\n",
    "        \n",
    "    return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which get the df and get the y value which is the label value. \n",
    "#It will also iterate over the text and apply the bag of words function to each text and return it as x. \n",
    "def df_xy(dataframe):\n",
    "    y = dataframe['Label']\n",
    "    \n",
    "    text = dataframe['Text']\n",
    "    vectors = []\n",
    "    \n",
    "    for t in text:\n",
    "        vector = bag_of_words(t)\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    x = np.array(vectors).astype(int)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42632, 774), (42632,), (10658, 774), (10658,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function will then be applied on train and test data.\n",
    "x_train, y_train = df_xy(train_df)\n",
    "x_test, y_test = df_xy(test_df)\n",
    "\n",
    "\n",
    "x_train.shape , y_train.shape , x_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> We will now get the accuracy using the three evalualtion methods and export the one with the highest accuracy. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(x_train , y_train)\n",
    "lrpred = lr.predict(x_test)\n",
    "lrscore =accuracy_score(y_test,lrpred)\n",
    "print(f'Accuracy: {round(lrscore*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5165,  208],\n",
       "       [ 321, 4964]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,lrpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.81%\n"
     ]
    }
   ],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(x_train, y_train)\n",
    "\n",
    "nbpred = NB.predict(x_test)\n",
    "nbscore=accuracy_score(y_test,nbpred)\n",
    "print(f'Accuracy: {round(nbscore*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4794,  579],\n",
       "       [ 614, 4671]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,nbpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.96%\n"
     ]
    }
   ],
   "source": [
    "#Initialize a PassiveAggressiveClassifier\n",
    "pac=PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(x_train,y_train)\n",
    "\n",
    "#Predict on the test set and calculate accuracy\n",
    "pacpred=pac.predict(x_test)\n",
    "pacscore=accuracy_score(y_test,pacpred)\n",
    "print(f'Accuracy: {round(pacscore*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4789,  584],\n",
       "       [ 273, 5012]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pacpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> As it can be seen, we get the highest accuracy using logistic regression so we export that. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier and mapping dictionary to a pickle file\n",
    "with open('../bagOfWords.pickle', 'wb') as f:\n",
    "    pickle.dump(lr, f)\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
