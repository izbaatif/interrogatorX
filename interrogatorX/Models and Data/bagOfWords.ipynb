{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCARY! LEAKED EMAIL PROVES Radical Billionaire...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch as Assad Destroys US Reporter Michael Is...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK counter-terrorism police charge 14-year-old...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Internet Drags Trump’s Son For Saying ‘Th...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Koch Has The Sads Because He Thinks H...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  SCARY! LEAKED EMAIL PROVES Radical Billionaire...  fake\n",
       "1  Watch as Assad Destroys US Reporter Michael Is...  fake\n",
       "2  UK counter-terrorism police charge 14-year-old...  real\n",
       "3   The Internet Drags Trump’s Son For Saying ‘Th...  fake\n",
       "4   Charles Koch Has The Sads Because He Thinks H...  fake"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save data in dataframe\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#get all the English stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which will first tokenize all the words in the Body field. \n",
    "#Then these words will be lemmatized to obtain the root words for example dance and dancing will be the same. \n",
    "#After that it will remove the stop words such as 'the' to only keep important words which are useful for the analysis. \n",
    "#Lastly, this function will return these words in a list.\n",
    "def preprocess_text(text):\n",
    "    # Check if text is a string or can be converted to a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Make tokens of everything in the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Make them all lowercase\n",
    "    lower = [text.lower() for text in tokens]\n",
    "    \n",
    "    # Lemmatize all the tokens and store them in a list\n",
    "    lemmatized = [lemmatizer.lemmatize(text) for text in lower]\n",
    "    \n",
    "    # Get all the words which aren't stopwords in a list\n",
    "    words = [text for text in lemmatized if text not in stopwords]\n",
    "    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 80:20 for train and test\n",
    "#find 80th percent of the length of the dataframe\n",
    "split = int(len(df) * 0.8)\n",
    "\n",
    "#split everything before that into train and the rest into test\n",
    "train_df, test_df = df[:split], df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125962\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary which stores each token and its count\n",
    "# Empty dictionary for storing the tokens\n",
    "token_count = {}\n",
    "\n",
    "# Tokenize the combined text\n",
    "for text in train_df['Text']:\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in token_count:\n",
    "            token_count[token] += 1\n",
    "        else:\n",
    "            token_count[token] = 1\n",
    "\n",
    "# Print the count of unique tokens\n",
    "print(len(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140366\n"
     ]
    }
   ],
   "source": [
    "#check max and min value\n",
    "print(max(token_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min(token_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which returns true if input word is appearing between the threshold\n",
    "def common_tokens(token, lowthresh, highthresh):\n",
    "    if token not in token_count:\n",
    "        return False\n",
    "    else:\n",
    "        tokens = token_count[token] > lowthresh and token_count[token] < highthresh\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital', 'press', 'act', 'getting', 'seek', 'front', 'trying', 'watch', 'city', 'market', 'backed', 'reporter', '2013', 'individual', 'statement', 'party', 'interest', 'six', 'spokesman', 'union', 'today', 'simply', 'cruz', 'hold', '12', 'continue', 'failed', 'son', 'thursday', 'seems', 'monday', 'oil', 'lie', 'accused', 'county', 'open', 'evidence', 'iraq', 'source', 'strong', 'increase', 'though', 'idea', 'friend', '2016', 'officer', 'money', 'decade', 'everything', 'europe', 'early', 'june', 'received', 'started', 'face', 'twitter', 'district', 'leader', 'court', 'com', 'question', 'biggest', 'someone', 'leave', 'doe', 'lot', 'immigration', 'comey', 'also', 'peace', 'request', 'known', 'china', '2015', 'yet', 'washington', 'isi', 'sander', 'america', 'agency', '2012', 'test', 'bernie', 'bush', 'august', 'job', 'attack', 'host', 'politician', 'office', 'wife', 'believe', 'rather', 'sunday', 'share', 'mean', 'policy', 'least', 'would', 'going', 'taken', 'british', 'al', 'cost', 'allegation', 'chief', 'citizen', 'effort', 'access', 'lawmaker', 'keep', 'cnn', 'family', 'sure', 'know', 'century', 'making', 'getty', 'hand', 'large', 'korea', 'level', 'defense', 'say', 'force', 'work', 'give', 'social', 'stand', 'ago', 'thing', 'country', 'candidate', 'tv', 'place', 'action', 'via', 'senator', 'close', 'potential', 'across', 'intelligence', 'met', 'needed', 'fight', 'wrong', 'video', 'business', 'better', '50', 'shot', 'law', 'feel', '25', 'back', '6', 'able', 'last', 'news', 'racist', 'old', 'putin', 'crowd', 'hard', 'j', 'made', 'matter', 'include', 'public', 'victim', 'comment', 'event', 'firm', 'body', 'long', 'day', 'little', 'much', 'debate', 'coming', 'target', 'final', 'using', '2', 'see', 'number', 'senate', 'saturday', 'poll', 'calling', 'rule', 'financial', 'measure', 'year', 'including', 'several', 'almost', 'medium', 'man', 'hit', 'line', 'shooting', 'militant', 'another', 'told', 'game', 'white', 'private', 'eu', 'operation', 'forward', 'whether', 'secret', 'two', 'role', 'budget', 'christian', 'investigation', 'care', 'article', 'announced', 'hour', 'daily', 'led', 'toward', 'hundred', 'document', 'global', 'tuesday', 'due', 'cannot', 'play', 'featured', 'actually', 'region', 'really', 'position', 'website', 'reuters', 'different', 'south', 'way', 'bill', 'entire', 'four', 'near', 'sign', 'allowed', 'october', 'email', 'second', 'ministry', 'ruling', 'moscow', 'relationship', 'list', 'james', 'chance', 'anyone', 'detail', 'past', 'tweet', 'talking', 'men', 'mexico', 'one', 'point', 'page', 'george', 'november', 'gun', 'expert', 'found', 'good', 'despite', 'economic', 'love', 'pay', 'got', 'issue', 'often', 'agreement', 'supporter', 'muslim', 'chairman', 'freedom', 'guy', 'deal', 'million', 'power', 'california', 'barack', 'history', 'third', 'example', 'protester', 'photo', 'think', 'time', '10', 'russian', 'important', 'bring', 'tie', 'fund', 'minister', 'according', 'however', 'town', 'information', 'came', 'account', 'national', 'r', 'israel', '4', 'coalition', 'within', 'likely', 'weapon', 'speech', 'former', 'speaking', 'east', 'united', 'thought', 'interview', 'change', 'chinese', 'friday', 'image', 'called', 'general', 'opposition', 'major', 'street', 'planned', 'area', 'either', 'kind', 'half', 'woman', 'although', 'university', 'people', 'january', 'john', 'hillary', '11', 'pressure', 'must', 'movement', 'price', 'voting', 'ever', 'democrat', 'trade', 'economy', 'story', '5', 'show', 'politics', 'become', 'get', 'rubio', 'service', 'human', 'house', 'earlier', 'ahead', 'employee', 'hearing', 'leading', 'russia', 'real', 'week', 'government', 'start', 'win', 'world', 'nuclear', 'big', 'syria', 'wednesday', 'nothing', '20', 'tax', 'along', 'convention', 'federal', 'expected', 'part', 'incident', 'plan', 'working', '000', 'morning', 'support', 'group', 'main', 'belief', 'military', 'co', 'book', 'congressional', 'le', 'lost', '2017', 'energy', 'concern', 'crime', 'turkey', 'many', 'come', 'violence', 'program', 'sexual', 'billion', 'course', 'benefit', 'since', 'recent', 'top', 'name', 'clear', 'life', 'want', 'gop', 'independent', 'paul', 'black', 'war', 'try', 'home', 'great', 'congress', 'response', 'administration', 'lead', 'activist', 'took', 'released', 'school', 'fighting', 'hate', 'without', 'star', 'rally', 'possible', 'special', 'speaker', 'key', 'recently', 'return', 'american', 'vote', 'healthcare', 'look', 'thousand', 'post', 'international', 'right', 'claim', '100', 'running', 'attempt', 'central', 'three', 'word', 'wall', 'among', '7', 'cause', 'result', 'child', 'ally', 'pic', 'end', 'began', 'review', 'later', 'reason', 'north', 'serious', 'west', 'anything', 'worked', 'student', 'asked', 'others', 'december', 'still', 'left', 'like', 'texas', 'system', 'secretary', 'campaign', 'taking', 'new', 'political', 'committee', 'well', 'run', 'executive', 'york', 'challenge', 'representative', 'border', 'risk', 'decision', 'authority', 'moment', 'go', 'obama', 'primary', '2014', 'already', 'truth', 'britain', 'parliament', 'ryan', 'facebook', 'building', 'adviser', 'center', 'charge', 'especially', 'supreme', 'allow', 'crisis', 'put', 'protest', 'justice', 'happened', '15', 'mr', 'room', 'head', 'fear', 'behind', 'police', 'based', 'missile', 'food', 'term', 'conference', 'threat', 'march', 'conflict', 'wrote', 'turn', 'nation', 'situation', 'obamacare', 'case', 'help', 'republican', 'letter', 'read', 'civil', 'staff', '30', 'high', 'flag', 'father', 'http', 'future', 'process', 'terrorist', 'call', 'immigrant', 'view', 'find', 'night', 'form', 'worker', 'even', 'looking', 'away', 'president', 'order', 'meet', 'reported', 'local', 'refugee', 'killed', 'funding', 'election', 'personal', 'industry', 'wanted', 'fact', 'fox', 'voted', 'talk', 'added', 'illegal', 'dollar', 'voter', 'vice', 'young', 'far', 'spending', 'korean', 'network', 'criminal', 'pro', 'nearly', 'sent', 'best', 'late', 'car', 'saying', 'current', 'tried', 'project', 'something', 'could', 'july', 'step', 'percent', 'short', 'fbi', 'provide', 'bank', 'used', 'together', '8', 'rate', 'latest', 'seen', 'realdonaldtrump', 'enforcement', 'went', 'five', 'need', 'fake', 'drug', 'donald', 'governor', 'prime', 'release', 'mark', 'victory', 'terrorism', 'ground', 'middle', 'climate', 'u', 'sen', 'death', 'involved', 'following', 'company', 'control', 'hope', 'sanction', 'opinion', 'state', 'community', 'live', 'travel', 'decided', 'showed', 'every', 'free', 'bad', 'anti', 'foreign', '9', 'liberal', 'first', 'april', 'council', 'non', 'nominee', 'conservative', 'might', 'everyone', 'instead', 'visit', 'member', 'problem', 'stop', 'ha', 'held', 'relation', 'presidential', 'move', 'iran', 'foundation', 'answer', 'whose', 'western', 'health', 'let', 'official', 'month', 'proposal', 'department', 'water', 'done', 'september', 'gave', 'n', 'syrian', 'set', 'judge', 'florida', 'nomination', 'low', 'data', 'next', 'democratic', 'reform', 'never', 'address', 'use', 'meeting', 'fire', 'islamic', 'make', 'race', 'agent', 'true', 'february', 'probably', 'air', 'legal', 'germany', 'ban', '3', 'small', 'cut', 'record', 'report', 'organization', 'director', 'outside', 'protect', 'side', 'always', 'elected', 'lawyer', 'saudi', 'european', 'message', 'full', 'person', 'army', 'security', '1', 'given', 'class', 'choice', 'majority', 'soon', 'aide', 'around', 'tell', 'enough', 'attorney', 'take', 'college', 'legislation', 'senior', 'team', 'may', 'presidency', 'immediately', 'session', 'clinton', 'board'}\n"
     ]
    }
   ],
   "source": [
    "#set to store all tokens apperaing inside threshold\n",
    "common = set()\n",
    "\n",
    "#loops through the token count dictionary \n",
    "for token in token_count:\n",
    "    #if the value if greater than 3000 and less than 100000 adds it \n",
    "    if common_tokens(token,3000, 100000):\n",
    "        common.add(token)\n",
    "        \n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital': 0, 'press': 1, 'act': 2, 'getting': 3, 'seek': 4, 'front': 5, 'trying': 6, 'watch': 7, 'city': 8, 'market': 9, 'backed': 10, 'reporter': 11, '2013': 12, 'individual': 13, 'statement': 14, 'party': 15, 'interest': 16, 'six': 17, 'spokesman': 18, 'union': 19, 'today': 20, 'simply': 21, 'cruz': 22, 'hold': 23, '12': 24, 'continue': 25, 'failed': 26, 'son': 27, 'thursday': 28, 'seems': 29, 'monday': 30, 'oil': 31, 'lie': 32, 'accused': 33, 'county': 34, 'open': 35, 'evidence': 36, 'iraq': 37, 'source': 38, 'strong': 39, 'increase': 40, 'though': 41, 'idea': 42, 'friend': 43, '2016': 44, 'officer': 45, 'money': 46, 'decade': 47, 'everything': 48, 'europe': 49, 'early': 50, 'june': 51, 'received': 52, 'started': 53, 'face': 54, 'twitter': 55, 'district': 56, 'leader': 57, 'court': 58, 'com': 59, 'question': 60, 'biggest': 61, 'someone': 62, 'leave': 63, 'doe': 64, 'lot': 65, 'immigration': 66, 'comey': 67, 'also': 68, 'peace': 69, 'request': 70, 'known': 71, 'china': 72, '2015': 73, 'yet': 74, 'washington': 75, 'isi': 76, 'sander': 77, 'america': 78, 'agency': 79, '2012': 80, 'test': 81, 'bernie': 82, 'bush': 83, 'august': 84, 'job': 85, 'attack': 86, 'host': 87, 'politician': 88, 'office': 89, 'wife': 90, 'believe': 91, 'rather': 92, 'sunday': 93, 'share': 94, 'mean': 95, 'policy': 96, 'least': 97, 'would': 98, 'going': 99, 'taken': 100, 'british': 101, 'al': 102, 'cost': 103, 'allegation': 104, 'chief': 105, 'citizen': 106, 'effort': 107, 'access': 108, 'lawmaker': 109, 'keep': 110, 'cnn': 111, 'family': 112, 'sure': 113, 'know': 114, 'century': 115, 'making': 116, 'getty': 117, 'hand': 118, 'large': 119, 'korea': 120, 'level': 121, 'defense': 122, 'say': 123, 'force': 124, 'work': 125, 'give': 126, 'social': 127, 'stand': 128, 'ago': 129, 'thing': 130, 'country': 131, 'candidate': 132, 'tv': 133, 'place': 134, 'action': 135, 'via': 136, 'senator': 137, 'close': 138, 'potential': 139, 'across': 140, 'intelligence': 141, 'met': 142, 'needed': 143, 'fight': 144, 'wrong': 145, 'video': 146, 'business': 147, 'better': 148, '50': 149, 'shot': 150, 'law': 151, 'feel': 152, '25': 153, 'back': 154, '6': 155, 'able': 156, 'last': 157, 'news': 158, 'racist': 159, 'old': 160, 'putin': 161, 'crowd': 162, 'hard': 163, 'j': 164, 'made': 165, 'matter': 166, 'include': 167, 'public': 168, 'victim': 169, 'comment': 170, 'event': 171, 'firm': 172, 'body': 173, 'long': 174, 'day': 175, 'little': 176, 'much': 177, 'debate': 178, 'coming': 179, 'target': 180, 'final': 181, 'using': 182, '2': 183, 'see': 184, 'number': 185, 'senate': 186, 'saturday': 187, 'poll': 188, 'calling': 189, 'rule': 190, 'financial': 191, 'measure': 192, 'year': 193, 'including': 194, 'several': 195, 'almost': 196, 'medium': 197, 'man': 198, 'hit': 199, 'line': 200, 'shooting': 201, 'militant': 202, 'another': 203, 'told': 204, 'game': 205, 'white': 206, 'private': 207, 'eu': 208, 'operation': 209, 'forward': 210, 'whether': 211, 'secret': 212, 'two': 213, 'role': 214, 'budget': 215, 'christian': 216, 'investigation': 217, 'care': 218, 'article': 219, 'announced': 220, 'hour': 221, 'daily': 222, 'led': 223, 'toward': 224, 'hundred': 225, 'document': 226, 'global': 227, 'tuesday': 228, 'due': 229, 'cannot': 230, 'play': 231, 'featured': 232, 'actually': 233, 'region': 234, 'really': 235, 'position': 236, 'website': 237, 'reuters': 238, 'different': 239, 'south': 240, 'way': 241, 'bill': 242, 'entire': 243, 'four': 244, 'near': 245, 'sign': 246, 'allowed': 247, 'october': 248, 'email': 249, 'second': 250, 'ministry': 251, 'ruling': 252, 'moscow': 253, 'relationship': 254, 'list': 255, 'james': 256, 'chance': 257, 'anyone': 258, 'detail': 259, 'past': 260, 'tweet': 261, 'talking': 262, 'men': 263, 'mexico': 264, 'one': 265, 'point': 266, 'page': 267, 'george': 268, 'november': 269, 'gun': 270, 'expert': 271, 'found': 272, 'good': 273, 'despite': 274, 'economic': 275, 'love': 276, 'pay': 277, 'got': 278, 'issue': 279, 'often': 280, 'agreement': 281, 'supporter': 282, 'muslim': 283, 'chairman': 284, 'freedom': 285, 'guy': 286, 'deal': 287, 'million': 288, 'power': 289, 'california': 290, 'barack': 291, 'history': 292, 'third': 293, 'example': 294, 'protester': 295, 'photo': 296, 'think': 297, 'time': 298, '10': 299, 'russian': 300, 'important': 301, 'bring': 302, 'tie': 303, 'fund': 304, 'minister': 305, 'according': 306, 'however': 307, 'town': 308, 'information': 309, 'came': 310, 'account': 311, 'national': 312, 'r': 313, 'israel': 314, '4': 315, 'coalition': 316, 'within': 317, 'likely': 318, 'weapon': 319, 'speech': 320, 'former': 321, 'speaking': 322, 'east': 323, 'united': 324, 'thought': 325, 'interview': 326, 'change': 327, 'chinese': 328, 'friday': 329, 'image': 330, 'called': 331, 'general': 332, 'opposition': 333, 'major': 334, 'street': 335, 'planned': 336, 'area': 337, 'either': 338, 'kind': 339, 'half': 340, 'woman': 341, 'although': 342, 'university': 343, 'people': 344, 'january': 345, 'john': 346, 'hillary': 347, '11': 348, 'pressure': 349, 'must': 350, 'movement': 351, 'price': 352, 'voting': 353, 'ever': 354, 'democrat': 355, 'trade': 356, 'economy': 357, 'story': 358, '5': 359, 'show': 360, 'politics': 361, 'become': 362, 'get': 363, 'rubio': 364, 'service': 365, 'human': 366, 'house': 367, 'earlier': 368, 'ahead': 369, 'employee': 370, 'hearing': 371, 'leading': 372, 'russia': 373, 'real': 374, 'week': 375, 'government': 376, 'start': 377, 'win': 378, 'world': 379, 'nuclear': 380, 'big': 381, 'syria': 382, 'wednesday': 383, 'nothing': 384, '20': 385, 'tax': 386, 'along': 387, 'convention': 388, 'federal': 389, 'expected': 390, 'part': 391, 'incident': 392, 'plan': 393, 'working': 394, '000': 395, 'morning': 396, 'support': 397, 'group': 398, 'main': 399, 'belief': 400, 'military': 401, 'co': 402, 'book': 403, 'congressional': 404, 'le': 405, 'lost': 406, '2017': 407, 'energy': 408, 'concern': 409, 'crime': 410, 'turkey': 411, 'many': 412, 'come': 413, 'violence': 414, 'program': 415, 'sexual': 416, 'billion': 417, 'course': 418, 'benefit': 419, 'since': 420, 'recent': 421, 'top': 422, 'name': 423, 'clear': 424, 'life': 425, 'want': 426, 'gop': 427, 'independent': 428, 'paul': 429, 'black': 430, 'war': 431, 'try': 432, 'home': 433, 'great': 434, 'congress': 435, 'response': 436, 'administration': 437, 'lead': 438, 'activist': 439, 'took': 440, 'released': 441, 'school': 442, 'fighting': 443, 'hate': 444, 'without': 445, 'star': 446, 'rally': 447, 'possible': 448, 'special': 449, 'speaker': 450, 'key': 451, 'recently': 452, 'return': 453, 'american': 454, 'vote': 455, 'healthcare': 456, 'look': 457, 'thousand': 458, 'post': 459, 'international': 460, 'right': 461, 'claim': 462, '100': 463, 'running': 464, 'attempt': 465, 'central': 466, 'three': 467, 'word': 468, 'wall': 469, 'among': 470, '7': 471, 'cause': 472, 'result': 473, 'child': 474, 'ally': 475, 'pic': 476, 'end': 477, 'began': 478, 'review': 479, 'later': 480, 'reason': 481, 'north': 482, 'serious': 483, 'west': 484, 'anything': 485, 'worked': 486, 'student': 487, 'asked': 488, 'others': 489, 'december': 490, 'still': 491, 'left': 492, 'like': 493, 'texas': 494, 'system': 495, 'secretary': 496, 'campaign': 497, 'taking': 498, 'new': 499, 'political': 500, 'committee': 501, 'well': 502, 'run': 503, 'executive': 504, 'york': 505, 'challenge': 506, 'representative': 507, 'border': 508, 'risk': 509, 'decision': 510, 'authority': 511, 'moment': 512, 'go': 513, 'obama': 514, 'primary': 515, '2014': 516, 'already': 517, 'truth': 518, 'britain': 519, 'parliament': 520, 'ryan': 521, 'facebook': 522, 'building': 523, 'adviser': 524, 'center': 525, 'charge': 526, 'especially': 527, 'supreme': 528, 'allow': 529, 'crisis': 530, 'put': 531, 'protest': 532, 'justice': 533, 'happened': 534, '15': 535, 'mr': 536, 'room': 537, 'head': 538, 'fear': 539, 'behind': 540, 'police': 541, 'based': 542, 'missile': 543, 'food': 544, 'term': 545, 'conference': 546, 'threat': 547, 'march': 548, 'conflict': 549, 'wrote': 550, 'turn': 551, 'nation': 552, 'situation': 553, 'obamacare': 554, 'case': 555, 'help': 556, 'republican': 557, 'letter': 558, 'read': 559, 'civil': 560, 'staff': 561, '30': 562, 'high': 563, 'flag': 564, 'father': 565, 'http': 566, 'future': 567, 'process': 568, 'terrorist': 569, 'call': 570, 'immigrant': 571, 'view': 572, 'find': 573, 'night': 574, 'form': 575, 'worker': 576, 'even': 577, 'looking': 578, 'away': 579, 'president': 580, 'order': 581, 'meet': 582, 'reported': 583, 'local': 584, 'refugee': 585, 'killed': 586, 'funding': 587, 'election': 588, 'personal': 589, 'industry': 590, 'wanted': 591, 'fact': 592, 'fox': 593, 'voted': 594, 'talk': 595, 'added': 596, 'illegal': 597, 'dollar': 598, 'voter': 599, 'vice': 600, 'young': 601, 'far': 602, 'spending': 603, 'korean': 604, 'network': 605, 'criminal': 606, 'pro': 607, 'nearly': 608, 'sent': 609, 'best': 610, 'late': 611, 'car': 612, 'saying': 613, 'current': 614, 'tried': 615, 'project': 616, 'something': 617, 'could': 618, 'july': 619, 'step': 620, 'percent': 621, 'short': 622, 'fbi': 623, 'provide': 624, 'bank': 625, 'used': 626, 'together': 627, '8': 628, 'rate': 629, 'latest': 630, 'seen': 631, 'realdonaldtrump': 632, 'enforcement': 633, 'went': 634, 'five': 635, 'need': 636, 'fake': 637, 'drug': 638, 'donald': 639, 'governor': 640, 'prime': 641, 'release': 642, 'mark': 643, 'victory': 644, 'terrorism': 645, 'ground': 646, 'middle': 647, 'climate': 648, 'u': 649, 'sen': 650, 'death': 651, 'involved': 652, 'following': 653, 'company': 654, 'control': 655, 'hope': 656, 'sanction': 657, 'opinion': 658, 'state': 659, 'community': 660, 'live': 661, 'travel': 662, 'decided': 663, 'showed': 664, 'every': 665, 'free': 666, 'bad': 667, 'anti': 668, 'foreign': 669, '9': 670, 'liberal': 671, 'first': 672, 'april': 673, 'council': 674, 'non': 675, 'nominee': 676, 'conservative': 677, 'might': 678, 'everyone': 679, 'instead': 680, 'visit': 681, 'member': 682, 'problem': 683, 'stop': 684, 'ha': 685, 'held': 686, 'relation': 687, 'presidential': 688, 'move': 689, 'iran': 690, 'foundation': 691, 'answer': 692, 'whose': 693, 'western': 694, 'health': 695, 'let': 696, 'official': 697, 'month': 698, 'proposal': 699, 'department': 700, 'water': 701, 'done': 702, 'september': 703, 'gave': 704, 'n': 705, 'syrian': 706, 'set': 707, 'judge': 708, 'florida': 709, 'nomination': 710, 'low': 711, 'data': 712, 'next': 713, 'democratic': 714, 'reform': 715, 'never': 716, 'address': 717, 'use': 718, 'meeting': 719, 'fire': 720, 'islamic': 721, 'make': 722, 'race': 723, 'agent': 724, 'true': 725, 'february': 726, 'probably': 727, 'air': 728, 'legal': 729, 'germany': 730, 'ban': 731, '3': 732, 'small': 733, 'cut': 734, 'record': 735, 'report': 736, 'organization': 737, 'director': 738, 'outside': 739, 'protect': 740, 'side': 741, 'always': 742, 'elected': 743, 'lawyer': 744, 'saudi': 745, 'european': 746, 'message': 747, 'full': 748, 'person': 749, 'army': 750, 'security': 751, '1': 752, 'given': 753, 'class': 754, 'choice': 755, 'majority': 756, 'soon': 757, 'aide': 758, 'around': 759, 'tell': 760, 'enough': 761, 'attorney': 762, 'take': 763, 'college': 764, 'legislation': 765, 'senior': 766, 'team': 767, 'may': 768, 'presidency': 769, 'immediately': 770, 'session': 771, 'clinton': 772, 'board': 773}\n"
     ]
    }
   ],
   "source": [
    "#The words are then added to a list and arranged and given an index of most to least appeared\n",
    "#convert to list\n",
    "common = list(common)\n",
    "\n",
    "#map them\n",
    "mapping = {t:i for t,i in zip(common, range(len(common)))}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> Now make a bag of words. Bag of words include the words and how many times it appears in a text. For example, in the text: \"this year trump won the elections at this time of the year.\" : </p>\n",
    "        <p> [\"trump\" , \"year\" , \"time\"] <br>[  1  ,   2   ,   1  ]\n",
    "        </p>\n",
    "        <p> We will make a function to create the bag of words now. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bag of words\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    #make an empty vector of the length of common list\n",
    "    count_vector = np.zeros(len(common))\n",
    "    \n",
    "    #get the tokens list and store them\n",
    "    proc_tokens = preprocess_text(text)\n",
    "    \n",
    "    #count them and add them to the vector\n",
    "    for token in proc_tokens:\n",
    "        #if not in common, ignore the tokens \n",
    "        if token not in common:\n",
    "            continue\n",
    "        #count the values\n",
    "        index = mapping[token]\n",
    "        count_vector[index] += 1\n",
    "        \n",
    "    return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which get the df and get the y value which is the label value. \n",
    "#It will also iterate over the text and apply the bag of words function to each text and return it as x. \n",
    "def df_xy(dataframe):\n",
    "    y = dataframe['Label']\n",
    "    \n",
    "    text = dataframe['Text']\n",
    "    vectors = []\n",
    "    \n",
    "    for t in text:\n",
    "        vector = bag_of_words(t)\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    x = np.array(vectors).astype(int)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42632, 774), (42632,), (10658, 774), (10658,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function will then be applied on train and test data.\n",
    "x_train, y_train = df_xy(train_df)\n",
    "x_test, y_test = df_xy(test_df)\n",
    "\n",
    "\n",
    "x_train.shape , y_train.shape , x_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> We will now get the accuracy using the three evalualtion methods and export the one with the highest accuracy. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.04%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(x_train , y_train)\n",
    "lrpred = lr.predict(x_test)\n",
    "lrscore =accuracy_score(y_test,lrpred)\n",
    "print(f'Accuracy: {round(lrscore*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5165,  208],\n",
       "       [ 321, 4964]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,lrpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.81%\n"
     ]
    }
   ],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(x_train, y_train)\n",
    "\n",
    "nbpred = NB.predict(x_test)\n",
    "nbscore=accuracy_score(y_test,nbpred)\n",
    "print(f'Accuracy: {round(nbscore*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4794,  579],\n",
       "       [ 614, 4671]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,nbpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.23%\n"
     ]
    }
   ],
   "source": [
    "#Initialize a PassiveAggressiveClassifier\n",
    "pac=PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(x_train,y_train)\n",
    "\n",
    "#Predict on the test set and calculate accuracy\n",
    "pacpred=pac.predict(x_test)\n",
    "pacscore=accuracy_score(y_test,pacpred)\n",
    "print(f'Accuracy: {round(pacscore*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4659,  714],\n",
       "       [ 221, 5064]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pacpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> As it can be seen, we get the highest accuracy using logistic regression so we export that. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier and mapping dictionary to a pickle file\n",
    "with open('../website/pickleFiles/bagOfWords.pickle', 'wb') as f:\n",
    "    pickle.dump(lr, f)\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
