{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5441839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8acf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCARY! LEAKED EMAIL PROVES Radical Billionaire...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch as Assad Destroys US Reporter Michael Is...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK counter-terrorism police charge 14-year-old...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Internet Drags Trump’s Son For Saying ‘Th...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Koch Has The Sads Because He Thinks H...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  SCARY! LEAKED EMAIL PROVES Radical Billionaire...  fake\n",
       "1  Watch as Assad Destroys US Reporter Michael Is...  fake\n",
       "2  UK counter-terrorism police charge 14-year-old...  real\n",
       "3   The Internet Drags Trump’s Son For Saying ‘Th...  fake\n",
       "4   Charles Koch Has The Sads Because He Thinks H...  fake"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save data in dataframe\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fb7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#get all the English stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f4e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to lemmatize and remove stopwords\n",
    "def preprocess_text(text):\n",
    "    # Check if text is a string or can be converted to a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Make tokens of everything in the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Make them all lowercase\n",
    "    lower = [text.lower() for text in tokens]\n",
    "    \n",
    "    # Lemmatize all the tokens and store them in a list\n",
    "    lemmatized = [lemmatizer.lemmatize(text) for text in lower]\n",
    "    \n",
    "    # Get all the words which aren't stopwords in a list\n",
    "    words = [text for text in lemmatized if text not in stopwords]\n",
    "    \n",
    "    #convert back to text\n",
    "    sentence = ' '.join(words)\n",
    "    \n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c5a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    scary leaked email prof radical billionaire do...\n",
       "1    watch assad destroys u reporter michael isikof...\n",
       "2    uk counter terrorism police charge 14 year old...\n",
       "3    internet drag trump son saying better patriot ...\n",
       "4    charles koch ha sads think influence election ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['Text'].apply(preprocess_text)\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96851f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fake\n",
       "1    fake\n",
       "2    real\n",
       "3    fake\n",
       "4    fake\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the labels\n",
    "labels=df.Label\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858342d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(text, labels, test_size=0.2, random_state=7)\n",
    "x_train = x_train.fillna('')  # Replace NaN values with empty strings\n",
    "x_test = x_test.fillna('')  # Replace NaN values with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c102aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.9, min_df=0.1)\n",
    "\n",
    "#Fit and transform train set, transform test set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test=tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5102c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.21%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression classifier\n",
    "lr=LogisticRegression()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "lr.fit(tfidf_train, y_train)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "lrpred=lr.predict(tfidf_test)\n",
    "score=accuracy_score(y_test,lrpred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568bfa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5035,  286],\n",
       "       [ 438, 4899]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,lrpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67728e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.69%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a multinomial naive bayes classifier\n",
    "nb=MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb.fit(tfidf_train, y_train)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "nbpred=nb.predict(tfidf_test)\n",
    "score=accuracy_score(y_test,nbpred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46377572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4635,  686],\n",
       "       [ 626, 4711]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,nbpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd2cb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.17%\n"
     ]
    }
   ],
   "source": [
    "#Initialize a PassiveAggressiveClassifier\n",
    "pac=PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(tfidf_train,y_train)\n",
    "\n",
    "#Predict on the test set and calculate accuracy\n",
    "pcpred=pac.predict(tfidf_test)\n",
    "score=accuracy_score(y_test,pcpred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd690bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5120,  201],\n",
       "       [ 633, 4704]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build confusion matrix\n",
    "confusion_matrix(y_test,pcpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f78db",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> As it can be seen, we get the highest accuracy using passive aggressive classifier so we export that. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7649cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b1ca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier and tfidf vector to a pickle file\n",
    "with open('../website/pickleFiles/tfidf.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "    pickle.dump(lr, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7078945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
