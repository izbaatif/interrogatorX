{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020c12d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\izbaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1332a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data in dataframe\n",
    "df = pd.read_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5e1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#get all the English stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a016092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to lemmatize and remove stopwords\n",
    "def preprocess_text(text):\n",
    "    # Check if text is a string or can be converted to a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Make tokens of everything in the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Make them all lowercase\n",
    "    lower = [text.lower() for text in tokens]\n",
    "    \n",
    "    # Lemmatize all the tokens and store them in a list\n",
    "    lemmatized = [lemmatizer.lemmatize(text) for text in lower]\n",
    "    \n",
    "    # Get all the words which aren't stopwords in a list\n",
    "    words = [text for text in lemmatized if text not in stopwords]\n",
    "    \n",
    "    #convert back to text\n",
    "    sentence = ' '.join(words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3953ab74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scary leaked email prof radical billionaire do...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watch assad destroys u reporter michael isikof...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk counter terrorism police charge 14 year old...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internet drag trump son saying better patriot ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>charles koch ha sads think influence election ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  scary leaked email prof radical billionaire do...  fake\n",
       "1  watch assad destroys u reporter michael isikof...  fake\n",
       "2  uk counter terrorism police charge 14 year old...  real\n",
       "3  internet drag trump son saying better patriot ...  fake\n",
       "4  charles koch ha sads think influence election ...  fake"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8debd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "train_df , test_df=train_test_split(df, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92f2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes each word\n",
    "tokenized_text = [text.split() for text in df['Text']]\n",
    "#creates a new word2vec model with size of 100\n",
    "word2vec_model = gensim.models.Word2Vec(tokenized_text, vector_size=1000, window=10, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834cb71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get average word vector representation\n",
    "#takes as input the list, pretrained model , unique words in models vocab and dimensionality of vectors\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    #creates a vector with 0s of the lenght of numfeatures\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    #nwords keep track of words\n",
    "    nwords = 0.\n",
    "    #iterate over list\n",
    "    for word in words:\n",
    "        #if word is in model, increase nword\n",
    "        if word in vocabulary:\n",
    "            nwords += 1\n",
    "            #add the word to the vector with 0s\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    #check if nwords is now greater than 0 and if so calculate average\n",
    "    if nwords > 0:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    #return the vector\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0784cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training data\n",
    "#empty array to store\n",
    "train_data = []\n",
    "#iterates over train df\n",
    "for _, row in train_df.iterrows():\n",
    "    #apply the function of text column and adds it to the array\n",
    "    features= average_word_vectors(row['Text'].split(), word2vec_model, word2vec_model.wv.key_to_index, 1000)\n",
    "    train_data.append(features)\n",
    "#make a new array with labels of train_df\n",
    "train_labels = train_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d035adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat process for test data\n",
    "#empty array to store\n",
    "test_data = []\n",
    "#iterates over train df\n",
    "for _, row in test_df.iterrows():\n",
    "    #apply the function of text column and adds it to the array\n",
    "    features= average_word_vectors(row['Text'].split(), word2vec_model, word2vec_model.wv.key_to_index, 1000)\n",
    "    test_data.append(features)\n",
    "#make a new array with labels of test_df\n",
    "test_labels = test_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d362e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure there are no negative values\n",
    "train_data = np.clip(train_data, 0, None)\n",
    "test_data = np.clip(test_data, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c02005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression classifier\n",
    "lr=LogisticRegression()\n",
    "lr.fit(train_data, train_labels)\n",
    "\n",
    "#Predict on the test set and calculate accuracy\n",
    "lrpred=lr.predict(test_data)\n",
    "score=accuracy_score(test_labels,lrpred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71c867a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5072,  249],\n",
       "       [ 286, 5051]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels,lrpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04ec7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.23%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a multinomial naive bayes classifier\n",
    "nb=MultinomialNB()\n",
    "nb.fit(train_data, train_labels)\n",
    "\n",
    "#Predict on the test set and calculate accuracy\n",
    "nbpred=nb.predict(test_data)\n",
    "score=accuracy_score(test_labels,nbpred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "758dc92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4506,  815],\n",
       "       [ 759, 4578]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels,nbpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bcc9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.78%\n"
     ]
    }
   ],
   "source": [
    "#Initialize a PassiveAggressiveClassifier\n",
    "pac=PassiveAggressiveClassifier()\n",
    "pac.fit(train_data, train_labels)\n",
    "\n",
    "#Predict on the test set and calculate accuracy\n",
    "pcpred=pac.predict(test_data)\n",
    "score=accuracy_score(test_labels,pcpred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87975d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5132,  189],\n",
       "       [ 474, 4863]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build confusion matrix\n",
    "confusion_matrix(test_labels,pcpred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6fa13",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <p> As it can be seen, we get the highest accuracy using logistic regression so we export that. </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c5ba1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d56cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier and word2vec model to a pickle file\n",
    "with open('../website/pickleFiles/word2vec.pickle', 'wb') as f:\n",
    "    pickle.dump(lr, f)\n",
    "    pickle.dump(word2vec_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b1ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2576fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
